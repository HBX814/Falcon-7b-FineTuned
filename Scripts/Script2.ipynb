{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install necessary libraries\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -qU datasets tensorboard rouge-score nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T11:21:45.040243Z","iopub.execute_input":"2025-01-15T11:21:45.040533Z","iopub.status.idle":"2025-01-15T11:22:58.089647Z","shell.execute_reply.started":"2025-01-15T11:21:45.040510Z","shell.execute_reply":"2025-01-15T11:22:58.088832Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ntensorflow 2.17.1 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.18.0 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge_score import rouge_scorer\n\n# Model ID\nmodel_id = \"tiiuae/falcon-7b\"\n\n# Quantization Config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True\n)\n\n# Load Model with 4-bit Precision\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    quantization_config=quantization_config,\n)\n\n# Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token  # Set pad token\n\n# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"query_key_value\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Apply LoRA configuration to the model\nmodel = get_peft_model(model, lora_config)\n\n# Print trainable parameters for verification\nprint(f\"Trainable parameters: {model.print_trainable_parameters()}\")\n\n# Load Dataset\ndataset_path = \"Cynaptics/persona-chat\"\ndataset = load_dataset(dataset_path, split=\"train\")\n\n# Define functions for prompt generation and tokenization\ndef generate_prompt(data_point):\n    return f\"\"\"\n    Persona B: {', '.join(data_point['persona_b'])}\n\n    Conversation:\n    {data_point['dialogue']}\n    <AI>: {data_point['reference']}\n    \"\"\".strip()\n\ndef preprocess_data(data_point):\n    full_prompt = generate_prompt(data_point)\n    tokenized = tokenizer(\n        full_prompt,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\",\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n    return {\n        \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n        \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n        \"labels\": tokenized[\"labels\"].squeeze(0),\n    }\n\n# Apply preprocessing\nprocessed_dataset = dataset.map(preprocess_data, batched=False)\nprocessed_dataset.save_to_disk(\"processed_dataset\")\n\n# Limit dataset size for faster training\nsmall_dataset = processed_dataset.select(range(500))\n\n# Set use_cache to False\nmodel.config.use_cache = False\nmodel.gradient_checkpointing_enable = False\n\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n\n# Training Arguments\ntraining_args = TrainingArguments(\n    auto_find_batch_size=True,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    bf16=True,\n    save_total_limit=4,\n    logging_steps=10,\n    output_dir=\"output_dir\",\n    save_strategy='epoch',\n    report_to=\"tensorboard\",\n    gradient_accumulation_steps=4,\n)\n\n# Data Collator with reduced max_length\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=True,\n    max_length=128,\n    label_pad_token_id=tokenizer.pad_token_id,\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    train_dataset=small_dataset,\n    args=training_args,\n    data_collator=data_collator,\n)\n\n# Train the Model\ntrainer.train()\n\n# Save the trained model and tokenizer\nsave_directory = \"fine_tuned_model\"\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"Model and tokenizer saved to {save_directory}\")\n\n# Function to download saved weights\ndef download_weights(path):\n    import shutil\n    shutil.make_archive(\"fine_tuned_model\", 'zip', path)\n    print(\"Weights saved and ready for download as fine_tuned_model.zip.\")\n\ndownload_weights(save_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T11:23:03.733006Z","iopub.execute_input":"2025-01-15T11:23:03.733358Z","iopub.status.idle":"2025-01-15T13:01:56.914499Z","shell.execute_reply.started":"2025-01-15T11:23:03.733329Z","shell.execute_reply":"2025-01-15T13:01:56.913683Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be5ddd83c154c5299eb84767d4d91d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f452a84fa6a4849be84f7ba71f38a45"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n- configuration_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68c3c23a0ae04432a35554c9f6280996"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n- modeling_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12b015697db5472a906ec15e7e2382ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb382334e234f89955591c3b2d93a7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"862db51319ce448a87012121e9d150c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e71e65bc04436daaf2091297541411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f722bb2847964f55abb1cb65ad56d24d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcb72a11ccb849a7b5a10b99e4ac6ef2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56aee63004b34819ab7bdd0dfc6a4cb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9464eb3a49244c04b0603de624e20c65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7be3c7c534d74321924a7ee3bb65e596"}},"metadata":{}},{"name":"stderr","text":"You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 4,718,592 || all params: 6,926,439,296 || trainable%: 0.0681\nTrainable parameters: None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d74ae94e8d8c4c38891be8ecf6eb8678"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/11.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a743942dd94f0f94a282bf06bcb22e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec646e56eb7e4fd88996f15dc8d1a3af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adb66af9069f45b48877a7a60dd0cb0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6992966d66ef4d408d0ffdf8f6728b71"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 1:30:09, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.848600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model and tokenizer saved to fine_tuned_model\nWeights saved and ready for download as fine_tuned_model.zip.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from rouge_score import rouge_scorer  # Import RougeScorer\nfrom nltk.translate.bleu_score import sentence_bleu\nimport torch\n\n# Initialize scorer for ROUGE evaluation\nscorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n\n# Define test samples for evaluation\ntest_samples = [\n    {\n        \"persona\": [\n            \"I am a 25-year-old software developer.\",\n            \"I love solving puzzles and reading sci-fi novels.\",\n            \"I have a pet cat named Luna.\"\n        ],\n        \"context\": \"Hi there! I think we met at the book club last week. What's your favorite genre?\",\n        \"reference\": \"I love science fiction novels. They're my favorite genre. What about you?\"\n    },\n    {\n        \"persona\": [\n            \"I am a 40-year-old history professor.\",\n            \"I enjoy hiking and cooking in my free time.\",\n            \"I recently started learning how to play the guitar.\"\n        ],\n        \"context\": \"Hello! I heard you recently took up a new hobby. Can you tell me more about it?\",\n        \"reference\": \"Yes, I recently started learning to play the guitar. It's been a fun and rewarding experience.\"\n    },\n]\n\n# Function to evaluate the model\ndef evaluate_model(model, tokenizer, test_samples):\n    total_bleu = 0\n    total_rouge_l = 0\n\n    for idx, sample in enumerate(test_samples):\n        persona = \"\\n\".join([f\"Persona: {line}\" for line in sample[\"persona\"]])\n        prompt = f\"\"\"\n        {persona}\n\n        Context: {sample['context']}\n\n        Response: (Provide a detailed and informative answer about your preferences, hobbies, or opinions.)\n        \"\"\"\n\n        # Tokenize input prompt\n        input_ids = tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=512\n        ).input_ids.to(model.device)\n\n        # Generate model responses using beam search for multiple outputs\n        with torch.no_grad():\n            output_ids = model.generate(\n                input_ids,\n                max_new_tokens=50,\n                num_beams=5,  # Using beam search to generate multiple candidates\n                no_repeat_ngram_size=2,  # Avoid repeating n-grams\n                top_p=0.9,\n                temperature=0.7,\n                early_stopping=True,\n                use_cache=False  # Disable caching for better performance in some cases\n            )\n\n        # Decode response and reference\n        response = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n        response = response[len(prompt):].strip()  # Remove the prompt part\n\n        # Post-process the response to clean up unwanted characters or format\n        response = response.replace(\"\\n\", \" \").strip()  # Remove any newlines\n        reference = sample[\"reference\"]\n\n        # Compute BLEU score\n        reference_tokens = [reference.split()]\n        response_tokens = response.split()\n        bleu = sentence_bleu(reference_tokens, response_tokens)\n\n        # Compute ROUGE-L score\n        rouge_scores = scorer.score(reference, response)\n        rouge_l = rouge_scores[\"rougeL\"].fmeasure\n\n        total_bleu += bleu\n        total_rouge_l += rouge_l\n\n        # Print results\n        print(f\"Test Sample {idx + 1}:\")\n        print(f\"Prompt:\\n{prompt.strip()}\")\n        print(f\"Reference Response:\\n{reference}\")\n        print(f\"Model Response:\\n{response}\")\n        print(f\"BLEU Score: {bleu:.4f}\")\n        print(f\"ROUGE-L Score: {rouge_l:.4f}\")\n        print(\"-\" * 80)\n\n    # Print average scores\n    avg_bleu = total_bleu / len(test_samples)\n    avg_rouge_l = total_rouge_l / len(test_samples)\n    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n    print(f\"Average ROUGE-L Score: {avg_rouge_l:.4f}\")\n\n# Run evaluation\nevaluate_model(model, tokenizer, test_samples)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T13:29:27.907618Z","iopub.execute_input":"2025-01-15T13:29:27.907907Z","iopub.status.idle":"2025-01-15T13:34:47.231704Z","shell.execute_reply.started":"2025-01-15T13:29:27.907886Z","shell.execute_reply":"2025-01-15T13:34:47.230671Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Test Sample 1:\nPrompt:\nPersona: I am a 25-year-old software developer.\nPersona: I love solving puzzles and reading sci-fi novels.\nPersona: I have a pet cat named Luna.\n\n        Context: Hi there! I think we met at the book club last week. What's your favorite genre?\n\n        Response: (Provide a detailed and informative answer about your preferences, hobbies, or opinions.)\nReference Response:\nI love science fiction novels. They're my favorite genre. What about you?\nModel Response:\nreturn self.get_response(response)      def get_persona(self, response):         persona = {}         for key, value in response.items():             if key in PERSONAS:                 persona[key]\nBLEU Score: 0.0000\nROUGE-L Score: 0.0000\n--------------------------------------------------------------------------------\nTest Sample 2:\nPrompt:\nPersona: I am a 40-year-old history professor.\nPersona: I enjoy hiking and cooking in my free time.\nPersona: I recently started learning how to play the guitar.\n\n        Context: Hello! I heard you recently took up a new hobby. Can you tell me more about it?\n\n        Response: (Provide a detailed and informative answer about your preferences, hobbies, or opinions.)\nReference Response:\nYes, I recently started learning to play the guitar. It's been a fun and rewarding experience.\nModel Response:\ncontext = self.get_context(persona, context)         return context      def get_response(self, persona, response):         if response.lower() == 'yes':             return \"Yes, I\nBLEU Score: 0.0000\nROUGE-L Score: 0.1053\n--------------------------------------------------------------------------------\nAverage BLEU Score: 0.0000\nAverage ROUGE-L Score: 0.0526\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 2-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 3-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}